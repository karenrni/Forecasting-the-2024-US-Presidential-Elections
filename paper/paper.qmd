---
title: "Forecasting the 2024 Election: Tight Margins and Swing State Uncertainty"
subtitle: "Harris Leads Trump by 0.8% as Voters Remain Divided"
author: 
  - Mariko Lee
  - Karen Riani
  - Cristina Su Lam
thanks: "Code and data are available at: https://github.com/karenrni/Forecasting-the-2024-US-Presidential-Elections"
date: today
date-format: long
abstract: "We forecast the 2024 U.S. presidential winner using state-level poll averages for Kamala Harris and Donald Trump, weighted by poll quality and sample size. Applying a Bayesian regression model, we find a slight 0.8% overall lead for Harris over Trump and Harris leading in all 7 swing states. We weigh the polls with aims to increase the influence of the most reliable data in shaping the forecast—limiting biased results and showing a clearer voter sentiment amidst a particularly competitive election. These results show who may be leading the elections via polls, giving campaigns the estimations they need and reinforcing public trust in the election process, especially as the outcome likely hinges on key swing states."
toc: true
fig_caption: yes
number-sections: true
bibliography: references.bib
format: pdf
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(kableExtra)
library(maps) 
library(knitr)
library(ggplot2)
library(modelsummary)
```

# Introduction

Amid the chaotic, high-stakes 2024 U.S. presidential election, forecasting models offer useful evaluations into voter sentiment as Americans face a tight race between top candidates Donald Trump and Kamala Harris. With the attempted assassination of Trump, Joe Biden’s endorsement of Harris, and recent crises such as hurricanes disrupting voter access in swing states, public opinion remains volatile [ @ABCNews2024_HurricaneImpact]. Key events like Trump’s policy proposals on social security and public safety, Harris’s advocacy for reproductive rights, and other rising debates on gun control have further polarized the electorate, making forecasting models essential in offering clarity amid this competitive landscape [@CBSNews2024_CampaignPromises; @HRW2022_RoeVWade].

Using presidential polling data from FiveThirtyEight, our Bayesian model’s estimand is the probability of support for Kamala Harris, weighted by pollster rating and sample size to improve the reliability of aggregated polling data. By refining the aggregation of polls through Bayesian regression, filtered from the reelection day past July 21st, 2024 and weighted by sample size and pollster rating, this study contributes to the growing body of forecast models—including media outlets’ own polls-of-polls and various Bayesian methods. Our model estimates only a slim 0.8% lead for Harris over Trump in the overall popular vote—a close margin that highlights the high uncertainty in public sentiment. In line with other forecasts that struggle to declare a clear frontrunner, this narrow lead reflects the volatility of the race and the importance of updated, representative polling data [@NYTimes2024_ElectionPolls].

This uncertainty emphasizes the significance of swing states in election forecasts—particularly states with nearly equal support for each party and a history of alternating between Democratic and Republican candidates in recent elections [@Bloomberg2024_KeyStates]. Existing research identifies seven critical swing states, with Pennsylvania standing out as a tipping point in more than 1 in 5 simulations [@AP2024_ElectionForecasts]. With 19 electoral votes, Pennsylvania could prove decisive in the event of a close race. Our estimates reveal Harris’ lead in each swing state, however they are slim leads. The competitive nature of these swing states shows the need for continued data updates to maintain forecast accuracy in a closely contested race.

By prioritizing higher-quality polls and focusing on data collected after Harris’s campaign announcement, the model seeks to mitigate biases and improve forecast reliability by using a polls-of-polls method over individual pollster data. This approach not only clarifies voter sentiment amid competing forecasts but also demonstrates the value of weighting variables such as pollster rating and sample size in enhancing prediction accuracy. While some states lack sufficient polling data—introducing potential limitations that could be improved with more data and model complexity—the overall findings provide strategic forecasts for decision-makers and reinforce public confidence in an election where no candidate holds a decisive lead. Appendix A provides a detailed assessment on The Washington Post, one of the polling methodologies used by prominent sources, including sampling approaches, weighting, and non-response handling strategies to ensure robust data quality.

The rest of this paper is structured as follows:  @sec-mydatasection presents the data sources and methodology, @sec-mymodel presents the forecasting model, then  @sec-myresults followed by  @sec-mydisc, which discusses the results and their implications.  @sec-mylimits concludes with a discussion of the limitations of the study and suggestions for future research, followed by @sec-appenx with The Washington Post’s pollster methodology and a budgeted idealized methodology.

# Data Analysis {#sec-mydatasection}

```{r}
#| include: false
#| warning: false
#| message: false

# Load Parquet file and model
clean_president_polls <- read_parquet("../data/02-analysis_data/clean_president_polls.parquet")
bayesian_model <- readRDS("../models/first_model.rds")

```

## Overview
To forecast the 2024 U.S. presidential election, this analysis uses state-level polling data for candidates Kamala Harris and Donald Trump. Our primary data source is FiveThirtyEight's 2024 Presidential Election Forecast Database [@FiveThirtyEight], which aggregates polling data from multiple organizations and provides quality ratings for each poll, helping to identify and weigh the most reliable sources. The dataset covers polls conducted from July 21, 2024—when Kamala Harris announced her reelection bid—through to November 3, 2024.

Data cleaning was conducted in R [@citeR] using several packages: tidyverse [@tidyverse], janitor [@janitor], and lubridate [@lubridate]. Duplicate rows were removed, and the dataset was filtered to include only polls for Harris and Trump. Observations with missing values in key variables such as 'sample size,' 'numeric grade,' and 'state' were excluded. A binary variable, 'Chosen Candidate' (1 for Harris, 0 for Trump), was created to indicate support for each candidate. After these adjustments, the dataset was refined from its original 17,765 entries to 2,367 observations. The primary variables of interest include pollster, numeric grade, state, sample size, percentage, and chosen candidate. A sample of the cleaned dataset is displayed in [@tbl-presidentpolls].

For visualization and analysis, the knitr [@knitr], ggplot2 [@ggplot2_wickham_2016], kableExtra [@kableExtra], map [@maps] packages were utilized to generate tables, graphs, and maps, presenting the data clearly and effectively. 
```{r}
#| label: tbl-presidentpolls
#| tbl-cap: Sample of President Polls Dataset
#| echo: false
#| warning: false
#| fig-align: center
clean_president_polls |>   
  select(pollster, numeric_grade, state, sample_size, pct, candidate_chosen) |> 
  head(5) |> 
  kable(
    col.names = c("Pollster", "Numeric Grade", "State", "Sample Size", "Percentage", "Chosen Candidate"),
    booktabs = TRUE
  )
```

[@tbl-summary] presents summary statistics of the three numerical variables used. The **Percentage Support** variable, representing the proportion of respondents supporting a candidate, has a mean of 47.25% with a standard deviation of 4.41%, ranging from a minimum of 25% to a maximum of 70%. This spread reflects varying levels of support for each candidate across different polls.

The **Sample Size** variable, indicating the number of respondents in each poll, has a mean of 930.92 with a substantial standard deviation of 550.61, indicating variability in poll sizes. The minimum sample size is 301, while the maximum reaches 6,526, highlighting the range in poll reliability, as larger samples tend to yield more accurate reflections of public sentiment.

Lastly, the **Numeric Grade** variable, which rates pollster quality, shows an average score of 2.28 with a standard deviation of 0.63, spanning from 0.5 to 3. This distribution suggests that the dataset includes a mix of polls with varying reliability, with higher-rated polls contributing more heavily to the overall forecast due to weighted adjustments (further discussed in @sec-mymodel).
```{r}
#| label: tbl-summary
#| tbl-cap: Summary Statistics
#| echo: false
#| warning: false
#| fig-align: center
# Summary statistics for pct, sample_size, and numeric_grade
summary_stats <- clean_president_polls %>%
  summarize(
    mean_pct = mean(pct, na.rm = TRUE),
    sd_pct = sd(pct, na.rm = TRUE),
    min_pct = min(pct, na.rm = TRUE),
    max_pct = max(pct, na.rm = TRUE),
    mean_sample_size = mean(sample_size, na.rm = TRUE),
    sd_sample_size = sd(sample_size, na.rm = TRUE),
    min_sample_size = min(sample_size, na.rm = TRUE),
    max_sample_size = max(sample_size, na.rm = TRUE),
    mean_numeric_grade = mean(numeric_grade, na.rm = TRUE),
    sd_numeric_grade = sd(numeric_grade, na.rm = TRUE),
    min_numeric_grade = min(numeric_grade, na.rm = TRUE),
    max_numeric_grade = max(numeric_grade, na.rm = TRUE)
  )

# Create a summary table in the desired format
summary_table <- data.frame(
  `Statistic` = c("Percentage Support", "Sample Size", "Numeric Grade"),
  `Mean` = c(
    format(round(summary_stats$mean_pct, 2), big.mark = ",", scientific = FALSE),
    format(round(summary_stats$mean_sample_size, 2), big.mark = ",", scientific = FALSE),
    format(round(summary_stats$mean_numeric_grade, 2), big.mark = ",", scientific = FALSE)
  ),
  `SD` = c(
    format(round(summary_stats$sd_pct, 2), big.mark = ",", scientific = FALSE),
    format(round(summary_stats$sd_sample_size, 2), big.mark = ",", scientific = FALSE),
    format(round(summary_stats$sd_numeric_grade, 2), big.mark = ",", scientific = FALSE)
  ),
  `Min` = c(
    format(summary_stats$min_pct, big.mark = ",", scientific = FALSE),
    format(summary_stats$min_sample_size, big.mark = ",", scientific = FALSE),
    format(summary_stats$min_numeric_grade, big.mark = ",", scientific = FALSE)
  ),
  `Max` = c(
    format(summary_stats$max_pct, big.mark = ",", scientific = FALSE),
    format(summary_stats$max_sample_size, big.mark = ",", scientific = FALSE),
    format(summary_stats$max_numeric_grade, big.mark = ",", scientific = FALSE)
  )
)

kable(summary_table, format = "pipe")
```

## Measurement and Limitations
Converting real-world electoral dynamics into data requires careful measurement of public sentiment towards presidential candidates for the 2024 U.S. presidential election. This dataset is compiled from polling data collected by established pollsters such as Siena/NYT and YouGov, employing diverse methodologies to gauge voter preferences. Key variables captured include the percentage of respondents favoring each candidate, sample sizes, and demographic information about the participants.

The measurement process begins with pollsters designing surveys aimed at accurately reflecting voter attitudes. To ensure a representative sample, stratified random sampling methods are employed, accounting for the electorate's diverse demographics. Poll responses are gathered through various channels, including phone interviews, online surveys, and face-to-face interactions, ultimately quantifying voter support as a percentage for each candidate.

After polling concludes, the dataset undergoes thorough cleaning and processing to rectify missing values and discrepancies. Each entry corresponds to a specific poll, providing details such as the date of the poll, the methodology used, and the percentage of support for each candidate. A critical component is the Numeric Grade, which ranges from 0.5 ("Poor") to 3.0 ("Exceptional"), indicating poll quality and reliability in capturing true public sentiment. Based on FiveThirtyEight’s methodology [@ABCNews2024_Methodology], this grading system prioritizes data from more rigorous and representative polls, thereby enhancing forecast accuracy.

To determine these Numeric Grades, FiveThirtyEight assesses factors like sampling methods, sample sizes, methodological transparency, and historical pollster accuracy. Polls with robust data collection practices and larger representative samples receive higher ratings, reflecting a greater likelihood of reliability. Conversely, lower ratings signify methodological concerns that could affect accuracy.

In conclusion, this dataset transforms complex voter opinions into structured data, assigning reliability metrics through the Numeric Grade to facilitate systematic analysis of public sentiment trends. This methodology aids in making more precise predictions about election outcomes. However, challenges such as sampling bias, survey design limitations, and non-response bias must be acknowledged, as they can impact data accuracy and emphasize the importance of careful interpretation of polling results.

## Outcome Variable
### Distribution of Polls and State-Level Candidate Preference
The chosen candidate—whether a poll favors Harris or Trump—serves as the outcome variable in this analysis. [@fig-candidate] shows the distribution of polls between the two candidates, Harris and Trump, revealing a nearly equal split in support, with 49.8% favoring Trump and 50.2% favoring Harris. This close distribution highlights a competitive polling landscape, suggesting that public opinion is almost evenly divided between the candidates.

In [@fig-candbystate], the state-level distribution of support further emphasizes this balance, with only slight variations in candidate preference across states. For instance, states like Maryland, Montana, Nevada, Ohio, and Utah show a marginally higher level of support for Harris, representing the Democratic party. Conversely, in states such as Florida, Minnesota, and Pennsylvania, there is slightly greater support for Trump, representing the Republican party. These minor differences indicate regional trends but reinforce the overall theme of a closely contested race.

```{r}
#| label: fig-candidate
#| tbl-cap: Distribution of Chosen Candidate
#| echo: false
#| warning: false
#| fig-align: center
ggplot(clean_president_polls, aes(x = factor(candidate_chosen), fill = factor(candidate_chosen))) +
  geom_bar(alpha = 0.7) +
  labs(title = "Distribution of Polls by Harris vs Trump", 
       x = "Chosen Candidate", 
       y = "Number of Polls",
       fill = "Chosen Candidate") +  # This sets the legend title
  scale_x_discrete(labels = c("0" = "Trump", "1" = "Harris")) +
  theme_classic() +
  scale_fill_manual(values = c("0" = "red", "1" = "blue"), labels = c("Trump", "Harris")) +
  geom_text(stat = "count", aes(label = scales::percent(..count../sum(..count..), 
                                                        accuracy = 0.2)), vjust = -0.5) 
```

```{r}
#| label: fig-candbystate
#| tbl-cap: Distribution of Candidate Preference by State
#| echo: false
#| warning: false
#| fig-align: center
ggplot(clean_president_polls, aes(x = state, fill = factor(candidate_chosen, labels = c("Trump", "Harris")))) +
  geom_bar(position = "fill") +
  labs(title = "Candidate Preference by State", x = "State", y = "Proportion", fill = "Chosen Candidate") +
  scale_fill_manual(values = c("red", "blue")) +
  theme_classic() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, margin = margin(t = 5), size = 8)
  )
```

## Predictor Variables
### Distribution of Sample Size
[@fig-sample] shows the distribution of sample sizes across polls, with a right-skewed pattern indicating that most polls have smaller sample sizes and a few larger outliers. The majority of polls range between 300 and 1,500 respondents, peaking around 1,000 respondents, while only a few reach the maximum sample size of 6,526, as noted in [@tbl-summary]. This highlights that larger sample sizes are relatively uncommon in the dataset.

The prevalence of smaller sample sizes suggests greater variability in the precision of poll estimates, as smaller samples generally introduce more uncertainty. In contrast, larger sample sizes provide more stable and reliable reflections of public sentiment. This distribution emphasizes the need to account for sample size in the analysis to ensure that more reliable, larger polls are given appropriate weight, thus enhancing the accuracy of public opinion estimates.
```{r}
#| label: fig-sample
#| tbl-cap: Distribution of Sample Size
#| echo: false
#| warning: false
#| fig-align: center
ggplot(clean_president_polls, aes(x = sample_size)) +
  geom_histogram(binwidth = 200, fill = "darkred", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Sample Size", x = "Sample Size", y = "Frequency") +
  theme_classic()
```

### Distribution of Percentage of Candidate Support by Chosen Candidate
[@fig-percentage]. The density plot shows the distribution of candidate support percentages for Donald Trump and Kamala Harris, with both peaking around the 50% mark, where Harris has a slightly higher concentration near this midpoint. The magenta overlap region highlights this balance, showing no significant advantage for either candidate.

A few outliers in both distributions show support above 60% or below 40%, representing occasional strong deviations, though these instances are rare. This pattern reflects a balanced public sentiment with minor variation between the candidates and points to a highly competitive race. The symmetry and overlap of the distributions reinforce the close competition between Trump and Harris, indicating an almost evenly divided public opinion across the dataset.
```{r}
#| label: fig-percentage
#| tbl-cap: Distribution of Candidate Support by Chosen Candidate
#| echo: false
#| warning: false
#| fig-align: center
ggplot(clean_president_polls, aes(x = pct, fill = candidate_name)) +
  geom_density(alpha = 0.5) +
  labs(title = "Candidate Support Distribution", x = "Percentage Support", y = "Density") +
  scale_fill_manual(values = c("blue", "red"), name = "Candidate") +
  theme_classic()
```

### Distribution of Pollster and Numeric Grades
[@fig-numeric] displays the distribution of Numeric Grades assigned to pollsters, as per FiveThirtyEight’s grading system, with each grade indicating poll reliability. The x-axis shows the Numeric Grades, ranging from 0.5 to 3.0, while the y-axis represents the frequency of each grade.

The chart reveals that most polls come from higher-rated pollsters, with the highest concentrations at grades 2.0 (298 polls) and 3.0 (386 polls). This distribution suggests that the dataset primarily comprises data from pollsters with moderate to high reliability, while lower-quality sources, such as those with grades 0.5 and 1.0, are less common.

Complementing this, [@fig-pollsters] shows the distribution of polls by individual pollsters, with bar colors indicating each pollster's Numeric Grade. Darker shades of blue represent higher reliability scores. Notably, pollsters such as Siena/NYT and Marquette Law School have the highest Numeric Grade of 3.0, marking them as highly reliable sources. In contrast, pollsters like Trafalgar Group and Patriot Polling, with scores of 0.7 and 1.1, respectively, are represented as lower-quality sources.

These insights are crucial, as they enable the model to assign weights based on pollster reliability, improving the accuracy of the analysis. By prioritizing data from higher-grade pollsters, the model can more effectively capture public sentiment, minimizing the impact of potential biases or inaccuracies from less reliable sources.
```{r}
#| label: fig-numeric
#| tbl-cap: Pollsters by Count and Numeric Grade
#| echo: false
#| warning: false
#| fig-align: center
ggplot(clean_president_polls, aes(x = numeric_grade)) +
  geom_bar(fill = "darkred", alpha = 0.7) +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5, size = 3) +
  labs(title = "Distribution of Numeric Grades", x = "Numeric Grade", y = "Frequency") +
  theme_classic()
```

```{r}
#| label: fig-pollsters
#| tbl-cap: Pollsters by Count and Numeric Grade
#| echo: false
#| warning: false
#| fig-align: center
# Calculate the number of polls for each pollster with more than 20 polls
pollsters_over_20 <- clean_president_polls %>%
  group_by(pollster) %>%
  filter(n() > 20) %>%
  summarize(
    num_polls = n(),
    avg_numeric_grade = mean(numeric_grade, na.rm = TRUE)
  )

# Plot 
ggplot(pollsters_over_20, aes(x = reorder(pollster, -num_polls), y = num_polls)) +
  geom_bar(stat = "identity", aes(fill = avg_numeric_grade), alpha = 0.7) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Numeric Grade") +
  labs(title = "Distribution of Polls by Pollster", x = "Pollster", y = "Number of Polls") +
  geom_text(aes(label = round(avg_numeric_grade, 2)), vjust = -0.5, size = 2.5) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

### Distribution of Polls and Candidate Support by State
[@fig-numberstate] displays the number of polls conducted in each U.S. state. Pennsylvania has the highest poll count, followed by other key swing states such as Arizona, Georgia, Michigan, and Wisconsin. The concentration of polls in these states likely reflects their strategic importance as battlegrounds where shifts in public opinion can be crucial. In contrast, states like Vermont, Wyoming, and West Virginia have far fewer polls, suggesting less emphasis on states with more predictable or less competitive outcomes.

[@fig-support] presents a map indicating the leading candidate by state, with blue for Kamala Harris, red for Donald Trump, and gray for states with insufficient polling data. The map reveals a geographic divide, with Trump leading in many central and southern states, while Harris is favored in parts of the Northeast and West. This distribution aligns with typical U.S. regional political trends, where certain areas have a historical leaning toward one party. The map provides a visual summary of these regional preferences, highlighting how candidate support varies across the country.
```{r}
#| label: fig-numberstate
#| tbl-cap: Number of Pollsters by State
#| echo: false
#| warning: false
#| fig-align: center
ggplot(clean_president_polls, aes(x = reorder(state, -table(state)[state]))) +
  geom_bar(fill = "darkblue") +
  labs(title = "Number of Polls by State", x = "State", y = "Count") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#| label: fig-support
#| tbl-cap: Candidate with Highest Support Percentage by State
#| echo: false
#| warning: false
#| fig-align: center
# Summarize data to find the candidate with the highest percentage in each state
state_winners <- clean_president_polls %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  group_by(state) %>%
  summarize(winner = candidate_name[which.max(pct)]) 

# Convert state names to lowercase to match map_data
state_winners$state <- tolower(state_winners$state)

# Load USA state map data
usa_states <- map_data("state")

# Merge the winners with the USA map data
merged_data <- merge(usa_states, state_winners, by.x = "region", by.y = "state", sort = FALSE, all.x = TRUE)
merged_data <- merged_data[order(merged_data$group, merged_data$order), ]

# Plot the election map
ggplot(data = merged_data, aes(x = long, y = lat, group = group, fill = winner)) +
  geom_polygon(color = "black", size = 0.3) + 
  labs(fill = "Winner") +
  scale_fill_manual(values = c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  coord_fixed(1.3) + 
  labs(title = "Leading Candidate by State") +
  theme_void() +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
  )
```

# Model {#sec-mymodel}
## Overview
This section presents a Bayesian model designed to predict voter support for Donald Trump and Kamala Harris during the 2024 U.S. presidential election. The model assesses the level of support for each candidate across 43 states, including key swing states. It outputs the likelihood that a voter will favor either candidate, utilizing variables such as pollster, state, and percentage of candidate support. Additionally, weights were incorporated into the model to account for variations in pollster reliability and sample size.

To conduct this analysis, the R packages rstanarm [@citeArm] and MLmetrics [@MLmetrics] were employed. The rstanarm package streamlines the Bayesian modeling process by providing tools for fitting generalized linear models using a Bayesian framework, while MLmetrics supplies various metrics for assessing model performance, including accuracy, F1 score, and area under the curve (AUC).

## Model Structure

* Mathematical Representation
$$
\begin{align*}
\text{logit}(y_i) &= \beta_0 + \beta_1 \times \text{Pct}_i + u_{\text{pollster}} + u_{\text{state}} \\
\end{align*}
$$ 

* Component Explanation
$$
- $y_i$: Binary outcome variable indicating whether a respondent supports Harris (1) or not (0).
- $\beta_0$: Intercept term representing the baseline log-odds of support for Harris when the percentage support is zero.
- $\beta_1$: Coefficient for the percentage support for Harris (Pct), reflecting how changes in support percentage affect the log-odds of selecting Harris.
- $u_{\text{pollster}}$: Random effect for the specific pollster, accounting for variability in support across different pollsters.
- $u_{\text{state}}$: Random effect for the state in which the poll is conducted, capturing regional variations in support.
$$

## Variable Selection 
In constructing the Bayesian model, careful consideration was given to the selection of predictor variables to ensure that the analysis accurately reflects the complexities of voter support.

* Percentage Support (Pct): This continuous variable is integral to the model as it captures the nuanced impact of varying levels of support for Kamala Harris. By utilizing percentage support as a continuous measure, the model allows for a more detailed examination of how incremental changes in support influence the likelihood of selecting Harris over Trump. This approach provides a richer understanding of voter behavior compared to categorizing support into discrete groups, which could obscure critical trends and insights.

* Random Effects: The decision to treat pollster and state effects as random variables recognizes the potential for unobserved heterogeneity in the data. Different pollsters may employ varied methodologies, which can lead to fluctuations in reported support levels that are not adequately accounted for by fixed effects alone. Similarly, geographical differences can impact voter preferences, making it essential to model state-level variability. By incorporating random effects for both pollster and state, the model effectively captures this variability, enhancing its robustness and validity. This approach acknowledges that the influences on voter support can differ significantly based on the context in which polling is conducted.

## Weights and Priors
The weights in this model are calculated using the formula:
$$
\text{weights} = \text{Numeric Grade} \times \left(\frac{\text{Sample Size}}{\text{Mean(Sample Size)}}\right)
$$
This approach ensures that more reliable polls—indicated by a higher Numeric Grade—carry greater influence in the analysis. By incorporating the sample size relative to the average sample size, this weighting system acknowledges that larger, more representative samples tend to yield more accurate reflections of public sentiment. This method effectively balances the impact of various polls based on their reliability and representation, leading to a more robust estimation of voter support.

Regarding the priors, we define them as follows:
$$
\text{priors} \sim \text{Normal}(0.5, 0.1)
$$
This prior is centered around 0.5 with a standard deviation of 0.1, reflecting the belief that the probabilities of support for both candidates should be approximately 50% in the absence of data. This choice of prior is particularly useful in situations where there is prior knowledge or belief about the expected voter support distribution. By using a normal prior, we allow for flexibility in the model, enabling the incorporation of prior information while avoiding overly restrictive assumptions.

## Model Validation and Justification
### Model Validation Methods
The model validation process involved out-of-sample testing, where the dataset was split into training and testing sets in a 70-30 ratio. Both the Bayesian and logistic models utilized the same features, with the key difference being that the logistic model did not include random effects for pollster and state. As seen in [@tbl-comparison]. The performance of each model was evaluated using three key metrics: F1 Score, Area Under the Curve (AUC), and Root Mean Square Error (RMSE). 
```{r}
#| label: tbl-comparison
#| tbl-cap: Comparison of Model Performance Metrics
#| echo: false
#| warning: false
#| fig-align: center
# Create a data frame with the comparison data
comparison_data <- data.frame(
  Model = c("Logistic Model", "Bayesian Model"),
  `F1 Score` = c(0.4217, 0.5343),
  AUC = c(0.4261, 0.5301),
  RMSE = c(0.5551, 0.5096),
  check.names = FALSE
)

# Use kable to format the table
kable(comparison_data)
```

Although the Bayesian model still shows room for improvement—evidenced by F1 Score and AUC values not close to 1 and a RMSE that indicates some prediction error—it outperforms the logistic model across all metrics. This suggests that the Bayesian model, which incorporates random effects to account for variability from different pollsters and states, offers a more nuanced understanding of voter support.

### Diagnostics
To validate the Bayesian model, several diagnostic checks were conducted to assess its robustness and ensure that underlying assumptions were satisfied. Key among these were model convergence assessments and posterior predictive checks. The posterior predictive checks provided a visual representation of how well the model's predictions align with the observed data.

The graph reveals a close overlap between the observed outcomes (\(y\)) and the replicated predictions (\(y_{\text{rep}}\)), demonstrating that the model effectively captures the distribution of voter support for Harris. This strong alignment suggests that the model not only fits the training data well but also generalizes effectively to new observations. The flexibility of the Bayesian framework further allows for the incorporation of prior information and consideration of variability due to pollster and state effects, enhancing the overall robustness of the model (Refer to the Appendix for a visual illustration).

### Model Justification
The choice of the Bayesian model is justified by its capacity to account for the random effects of pollsters and states, providing a richer understanding of public sentiment. This model allows for nuanced insights that a simpler logistic regression might miss, particularly in a context where polling data is subject to variation due to different survey methods and demographic factors. Ultimately, while both models exhibit imperfections, the Bayesian model’s superior performance metrics support its selection as the final model for predicting voter support in the 2024 U.S. presidential election.

# Results {#sec-myresults}

Figure X shows the national predicted popular vote percentages for Harris (50.42%) and Trump (49.58%), calculated by averaging the mean predictions across all states. This aggregation reveals a very narrow 0.84% lead for Harris, highlighting a highly competitive race. This reveals a lack of dominance in candidacy and general uncertainty of pollster outcomes, suggesting that even this slight lead may not be substantial.

Additionally, Figure Y presents a heat map of Harris's predicted probability by state, illustrating her lead over the majority of states but revealing that this advantage is generally weak, with most not peaking over a 1% advantage. Most states show only a slight preference for Harris over Trump, with the margin of error spanning both candidates in many cases. Specifically, out of the 43 states with available data, 25 states lean toward Harris, yet none of these states show lead of greater than XX over Trump. This pattern additionally shows the substantial closeness of the race, as evidenced by the margin of error, where overlapping intervals highlight the slim advantage and limited certainty in Harris's predicted support.

Figure Z zooms in on the seven swing states, each of which shows a slight lean toward Harris. The graph includes a red dashed line at the 50% threshold, and although Harris is estimated to lead in these swing states, the margin is extremely tight, with most probabilities hovering just above 50%. Only Michigan and North Carolina show a slight lead of just over 1%, reinforcing the sensitivity of these predictions. This pattern suggests that even small shifts in voter sentiment could tip these critical states in either direction.

Looking closer at the margin of error, in figure X we can see Harris’s predicted probabilities by state. States to the right of the dashed line (above 0.5) lean towards Harris, while those to the left lean towards Trump. In swing states like North Carolina, Michigan, and Pennsylvania, Harris’s minimal lead is coupled with error bars that overlap this threshold. This suggests a high level of uncertainty—small shifts could easily change the lead in these states. Harris has a slight edge, but in many states, this lead is within the margin of error, thus showing that the election remains too close to call.

Figure W provides a summary of the model’s coefficients and fit statistics, capturing key model parameters like the intercepts, predictor estimates, and evaluation metrics. The negative intercept of -2.07 represents the baseline log odds for Harris, establishing the prediction's starting point in the absence of other predictors. With all intercepts being equal, there is initially a lower probability for Harris’s support without accounting for additional factors of percentage (pct) of support or pollster influence. The non-significant coefficient for sample size indicates that it does not directly affect prediction but serves as part of the poll weighting. In contrast, the pct coefficient of 0.04 suggests a slight positive effect on the odds for Harris, meaning an increase in her polling percentage slightly raises her predicted support odds. Low variance estimates for pollster and state effects imply limited inter-group variability, indicating that neither adds substantial random variation to the results.

Regarding model evaluation, the WAIC of 7072.4 and log likelihood of -3518.4 do not offer much information alone however a higher likelihood and a lower WAIC is preferred. The RMSE of 0.5 points to moderate accuracy, which aligns with the close and uncertain nature of the election. Additionally, a low ICC value suggests limited random variance contributed by pollster and state, furthermore showing the model’s constrained precision in forecasting a definitive winner.

```{r}
#| label: fig-summary-mod
#| tbl-cap: "Distribution of Sample Size"
#| echo: false
#| warning: false
#| fig-align: center

# Display model summary
modelsummary(bayesian_model, output = "kableExtra")
```

```{r}
```

```{r}
#| label: fig-national-vote
#| fig-cap: "National Popular Vote Prediction"

# Calculate overall percentage for Harris and Trump
overall_percentages <- data.frame(
  Candidate = c("Harris", "Trump"),
  Percentage = c(overall_percentage_harris, overall_percentage_trump)
)

# Plot national popular vote prediction
ggplot(overall_percentages, aes(x = Candidate, y = Percentage, fill = Candidate)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = sprintf("%.2f%%", Percentage)), vjust = -0.5) +
  labs(title = "National Popular Vote Prediction", y = "Percentage", x = "Candidate") +
  scale_fill_manual(values = c("blue", "red")) +
  theme_classic()

```

```{r}
# Plot state-level predictions for Harris and Trump
ggplot(state_predictions, aes(x = reorder(state, avg_predicted_prob_harris), y = avg_predicted_prob_harris, fill = state_winner)) +
  geom_bar(stat = "identity") +
  labs(title = "State-Level Predicted Support for Harris and Trump", x = "State", y = "Average Predicted Probability for Harris") +
  scale_fill_manual(values = c("blue", "red")) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
# Modify your state predictions dataframe to label swing states
swing_states <- c("Michigan", "Pennsylvania", "Wisconsin", "Arizona", "Georgia", "North Carolina", "Nevada")
state_predictions <- state_predictions %>%
  mutate(swing_state = ifelse(state %in% swing_states, "Swing State", "Non-Swing State"))

# Plot with enhancements
ggplot(state_predictions, aes(x = reorder(state, avg_predicted_prob_harris), y = avg_predicted_prob_harris, fill = swing_state)) +
  geom_bar(stat = "identity", color = "black", alpha = 0.8) +
  scale_fill_manual(values = c("Swing State" = "darkblue", "Non-Swing State" = "lightgrey")) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 0.8) +
  labs(
    title = "Predicted Probability of Harris Winning by State",
    x = "State",
    y = "Predicted Probability for Harris",
    fill = "State Type"
  ) +
  annotate("text", x = 1, y = 0.51, label = "0.5 Threshold (Neutral)", color = "red", hjust = 0) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()


```
```{r}
# Calculate the standard error and margin of error for Harris's predicted probability in each state
state_margins <- clean_president_polls %>%
  group_by(state) %>%
  summarize(
    avg_predicted_prob_harris = mean(predicted_prob_harris),
    se_predicted_prob_harris = sd(predicted_prob_harris) / sqrt(n()),
    margin_of_error = 1.96 * se_predicted_prob_harris
  ) %>%
  mutate(
    lower_bound = avg_predicted_prob_harris - margin_of_error,
    upper_bound = avg_predicted_prob_harris + margin_of_error
  )
ggplot(state_margins, aes(x = reorder(state, avg_predicted_prob_harris), y = avg_predicted_prob_harris)) +
  geom_point(color = "blue") +
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.2, color = "blue") +
  labs(
    title = "State-Level Predicted Probability for Harris with Margin of Error",
    x = "State",
    y = "Predicted Probability for Harris"
  ) +
  coord_flip() +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  labs(subtitle = "Dashed line represents a 50% threshold")


```

# Discussion {#sec-mydisc}

In a highly competitive election, accurately forecasting outcomes requires more than just raw polling data—it demands careful aggregation and weighting to capture true voter sentiment. 
With this in mind, our model estimates a narrow 0.8% lead for Kamala Harris in the national popular vote, resulting in an aggregated support of approximately 50.4% across states. 

While this suggests a slight edge for Harris, the lead easily falls within a \pm 3\% margin that reflects substantial uncertainty [@PewResearch_MarginOfError]. When a candidate’s lead is within this margin, the difference may not be statistically significant, meaning the true preference could feasibly favor either candidate. In this context, a 0.8% lead falls well within this range and could be due to sampling noise rather than a clear voter preference. In close races, even minor shifts in voter sentiment or turnout in key states could alter the outcome, showing the need for monitoring closely divided swing states and precise state-level analysis. 

Our model, through poll aggregation, weighted by pollster reliability and sample size, refines polling data into a more realistic prediction, helping offset biases in individual polls. This is because the weighting higher-quality polls reduces the influence of less reliable data seen in unweighted or individual poll estimates, providing a clearer view of voter sentiment. This approach, especially in closely contested swing states, increased the model’s sensitivity to shifts in these pivotal regions, reflecting on their power to overturn the elections while having a historically uncertain favourable candidate. 

In the 7 swing states– Arizona, Wisconsin, North Carolina, Pennsylvania, Georgia, Michigan and Nevada, our model shows Harris leading in each, with only 2 swing states showing a lead greater than 1%. Furthermore, the small margins seen in FUGURENARGIN show swing state leads ithin their respective margin of errors, pushing the uncertainty of the estimated lead. Other forecasts like FiveThirtyEight’s and The NYTimes’ offer similar leads for Harris [@FiveThirtyEight; nytimes2024_estimate]. When independent models using varied methodologies predict a similar outcome, it can suggest a trend that might hold under different sampling assumptions or adjustments. This convergence across forecasts further emphasizes the tightness of the race however, they can enhance confidence in a potential Harris lead, although it remains small.

The results are can additionally be hard to anticipate due to various unaccounted biases and inaccurate model adjustments. Trump may simply overturn his polls despite Harris’ slight forecasted leads due to shy voters, where people don’t publicly identify as conservative due to the negative social stigmas around it. What is more likely is an overturn due to the non-response bias. Republicans tend to be less responsive when it comes to polls which can lead to an unrepresentative dataset despite reweighed models altered to account for poor levels of distributed surveys. As there is no guarantee that the altered models work, it is not a confident show of prediction. Harris may also experience a large overturn due to similar inaccurately adjusted estimates in attempts to mitigate seemingly nonrepresentative data to an accurate prediction. However there the election may face incorrect election results due to small surveying or sampling errors, which can be devastating due to the tightness of the race [@NYT_ElectionPollsOpinion].

# Limitations and Next Steps {#sec-mylimits}
This analysis identifies several limitations that may impact the reliability of the predictions. First, the logistic model shows high levels of multicollinearity between state and pollster variables. This multicollinearity may inflate standard errors, complicating the interpretation of coefficients and impacting the model's accuracy. Addressing this issue in future analyses could enhance interpretability and offer a clearer comparison to the Bayesian and other models.
Additionally, environmental disruptions, such as recent hurricanes in key swing states, may introduce biases. These events affect both public sentiment and access to polling, potentially skewing results. While the state-level margin of error partially reflects this uncertainty, it also shows the model’s sensitivity to external societal influences that are challenging to control.
Maine and Nebraska, which allocate some electoral votes by congressional district, are simplified in this analysis by grouping each state into a single unit. While this approach captures broader state trends, it could underrepresent district-specific voting patterns, and future studies could address district-level data.
The model is fairly simple in comparison to other forecasts and does not currently adjust for other traditional variables or bias (i.e. partisianship) as polling sources, which could lead to an unbalanced or less accurate view of candidate support [@cambridge_polling_factors_2024]. Incorporating adjustments for pollster partisanship would improve accuracy by reflecting a more comprehensive cross-section of public opinion.
Another simplification in this analysis is the treatment of Maine and Nebraska, where data were aggregated at the state level instead of by congressional district. While this approach provides an overall view of candidate support in these states, it overlooks district-specific dynamics that could impact electoral vote allocation. This simplification streamlines the model but may underrepresent nuanced trends within these states.
Our method of weighting the polls does not have any constraints and could lead to the overweighting of certain data groups. If one sample size was much bigger than the majority, its weight would be extreme as well, potentially overcompensating for its affects. 
Finally, data gaps remain a significant factor, as polling data for several states were unavailable. Although these states are not swing states, the lack of data introduces uncertainty and potentially limits the model’s representativeness. This sparsity in effect with Harris’ recent campaign declaration, contributes to an overall margin of error. 
Future Directions
Future models could benefit from greater complexity to address these limitations. Incorporating additional influential factors, such as poll methodology (e.g., online versus in-person surveys), and pollster partisanship, would add depth to the analysis. Multi-level regression and post-stratification approaches, commonly used in polls-of-polls models, have shown to be effective in forecasting elections by capturing uncertainty more comprehensively [@citeTS]. Increasing model complexity with these factors could improve forecast reliability, particularly in an election as competitive as this one.
Integrating historical state-level polling data from previous elections could help fill data gaps and provide stronger context for recent trends. This is especially relevant given Biden’s late endorsement of Harris, where past polling data could help address the deficit of recent, state-level Democratic polling information. While this model focuses on the national popular vote, future work could benefit from projections of electoral votes, as these hold greater relevance in determining the election outcome.
In practice, reducing the influence of large polls is essential to avoid disproportionately weighting them. Implementing a cap on sample sizes, along with winsorizing extreme values, would prevent larger surveys from skewing results. Similarly, adjusting the weights of pollsters that release multiple polls in a short time frame would balance their impact, ensuring no single pollster overwhelms the dataset’s representation [@FiveThirtyEight_Poll_Avg ].



\newpage

\appendix

# Appendix {#sec-appenx}

# Methodology Analysis of The Washington Post Polling  
With an evaluation of sampling methodology, recruitment, handling non-response, and questionnaire design, this appendix offers an analysis of the polling methodology used by The Washington Post in collaboration with ABC News. The objective is to analyze these approaches' strengths and weaknesses and determine how they affect polling accuracy.

## Population, Frame, and Sample  
The Washington Post, in partnership with ABC News, employs a combination of text-to-web polls and random digit dialing (RDD) for landlines and mobile phones to reach a large and representative sample of American adults and registered voters [@WashPost2024_ABCMethodology].

With an evaluation of sampling methodology, recruitment, handling non-response, and questionnaire design, this appendix analyzes the polling methodology used by The Washington Post in collaboration with ABC News. The objective is to assess the strengths and weaknesses of these approaches and understand their impact on polling accuracy.  

## Population, Frame, and Sample  
The target population for The Washington Post and ABC News polls includes U.S. adults and registered voters. The sampling frame, or list from which the sample is drawn, includes landlines, mobile phones, and internet users, covering a broad demographic range. Using a combination of text-to-web polls and random digit dialing (RDD) for landlines and mobile phones, they aim to reach a probability sample where each individual in the population has a known chance of selection. This minimizes sampling bias and improves the representativeness of the sample [@WashPost2024_ABCMethodology]. 


The Washington Post's polling averages use only national and state-level polls that comply with strict quality and transparency criteria. These surveys were chosen because they employ suitable stratification and weighting strategies in addition to random sample approaches [@WashPost2024_PollingAverages]. To represent key demographics such as age, race, gender, and education, the samples are weighted [@WashPost2024_ABCMethodology] [@WashPost2024_PollingAverages].

## Sample Recruitment  
Data collection includes live phone interviews and text-to-web surveys, ensuring diverse demographic representation. In a typical 2024 poll, sample collection consisted of 21% text-to-web invites, 15% landlines, and 64% mobile phones [@WashPost2024_ABCMethodology]. This technique facilitates access to younger and minority voters, who may not be well represented in conventional landline-based surveys. 

ABC News also uses probability-based recruiting through the Ipos KnowledgePanel by using address-based sampling from the Delivery Sequence File of the US Postal Service. Since internet connections and equipment are offered at no cost, this guarantees that even households without internet connections or digital devices are involved [@ABCNews2024_Methodology]. 

## Sampling Approaches and Trade-offs
The Washington Post uses stratified random sampling to ensure that important demographics are represented proportionately to their voter base. Stratified sampling increases the likelihood of a balanced representation, while weighting addresses over- or under-representation of particular groups [@WashPost2024_ABCMethodology] [@WashPost2024_PollingAverages].

When state-level polling data is scarce, The Washington Post incorporates historical voting records from the last two presidential elections [@WashPost2024_PollingAverages]. This adjustment helps estimate current preferences but depends on past trends, which may miss recent shifts in voter sentiment [@WashPost2024_ABCMethodology].

## Non-response Handling  
Non-response can introduce non-response bias, where the views of non-respondents may differ from respondents, potentially skewing results. The Washington Post addresses this issue using response weighting based on age, race, and education to align the final sample with the overall population distribution [@WashPost2023_Standard].

ABC News also addressed non-response bias by applying post-stratification adjustments and sending email reminders to non-respondents. In addition, The Washington Post and ABC News ensure that their samples are weighted to account for any anomalies in non-response [@ABCNews2024_Methodology] [@WashPost2023_Standards].

Despite these initiatives, non-response bias is still a concern, especially for populations that are less inclined to take part in surveys, including younger or less politically active people [@WashPost2023_Standards].

## Questionnaire Design  
The Washington Post creates its surveys with neutrality and clarity to avoid influencing responses. Questions are rotated to minimize question order bias, and multiple-choice options, including “No opinion,” are provided [@WashPost2024_ABCMethodology]. Randomizing questions prevents any unintentional influence on how respondents interpret and answer follow-up questions [@WashPost2023_Standards]. 

ABC News follows similar principles, offering surveys in both Spanish and English to ensure inclusivity. Leading questions are purposefully omitted from the questionnaires to better capture genuine public opinion across language barriers [@ABCNews2024_Methodology]. 

## Strengths and Weaknesses of the Methodology  

**Strength:** \
**Sampling Method:** The combination of RDD, text-to-web polls, and live phone interviews enable access to a wide demographic, including younger and harder-to-reach voters [@WashPost2024_ABCMethodology] [@WashPost2023_Standards]. \
**Post-stratification Weighting:** Effective weighting adjusts for demographic imbalances, enhancing poll accuracy and representativeness [@WashPost2024_ABCMethodology] [@ABCNews2024_Methodology]. \
**Transparent Approach:** Using only high-quality polls and transparency in methodology increases the credibility of The Washington Post’s polling data [@WashPost2023_Standards] [@WashPost2024_PollingAverages]. \

**Weaknesses:** \
**Non-response Bias:** Despite response weighting, non-response remains an issue, particularly with groups less inclined to participate, such as younger individuals [@WashPost2023_Standards] [@WashPost2024_ABCMethodology]. \
**Dependency on Historical Data:** In states with limited polling, depending on past election data may not accurately reflect recent shifts in voter preferences [@WashPost2024_PollingAverages]. \

## Conclusion  
The Washington Post and ABC News polling methodologies offer a framework for measuring public opinion in the 2024 U.S. presidential election. Employing various sampling strategies, stratification, and weighting improves sample representation. However, challenges such as non-response bias and reliance on historical data in under-polled states need continued adjustments to maintain polling reliability. 

# Idealized Survey & Methodology - $100K Budget

## Overview
With a budget of $100K, this appendix outlines the survey methodology designed to predict the 2024 U.S. Presidential Election. The goal is to collect representative, high-quality data using recruiting, poll aggregation, and selective sample methods. This approach aims to minimize errors commonly encountered in survey research and to produce reliable insights. 

## Sampling Approach
We will use stratified random sampling to ensure fair representation across key demographic and geographic subgroups. This approach reduces bias and provides a more accurate view of voter preferences.

**Stratification Criteria:**  
- Age Groups  
- Gender  
- Education Levels  
- Geographic Regions  
- Political Affiliation

**Sample Size Goal:**   
- 10,000 respondents across states and demographics to achieve **high statistical power** with a margin of error below ±1%

**Trade-offs:**  
- While stratified sampling improves representativeness, it requires precise demographic data, which can increase costs. 

**Missing Data:**  
- Specific demographics (e.g., younger males) may have lower response rates. To address this, we will use **post-stratification weighting** and **data imputation**. 

## Recruitment Strategy
To ensure a broad demographic reach, we will combine **online recruitment** and **random-digit dialing (RDD)** for telephone surveys. This approach targets both urban and rural populations, increasing inclusivity. 

**Online Recruitment:**\
- Target ads on Google, Facebook, and Twitter to engage younger voters and urban populations. \
- Budget Allocation: $25,000

**Random-Digit Dialing (RDD):**\
- Phone surveys to engage older and rural voters with limited internet access. \
- Budget Allocation: $30,000

**Incentives:**\
- $5 gift cards offered to participants to increase response rates. 
- Budget Allocation: $20,000

**Non-Response Handling:**\
- Increase incentives for underrepresented groups and send multiple follow-up reminders. 

## Data Validation
To ensure data quality, several validation techniques will be implemented:

**Survey Logic Check:**\
- Identifies and flags contradictory responses (e.g., respondents under 18 claiming voter registration). \
**Attention Check:**\
- Includes questions to confirm active engagement (e.g., “Select ‘Confirm’ to proceed”) \
**Post-Stratification Weighting:**\
- Adjusts the sample for over- and under-enumeration and weighting to reflect the demographic composition of the U.S. population.

**Mode and Measurement Errors:**\
- Enumerators will be trained to reduce inconsistencies between online and phone responses, and survey questions will be straightforward to minimize misreporting. 

## Poll Aggregation Methodology
A **poll-of-polls aggregation** method will be applied to reduce bias and balance fluctuation between individual polls. 
**Weighting Criteria:**  
- **Sample Size:** larger samples receive higher weight for greater reliability.  
- **Recency:** More recent polls are given higher weight to capture modern voter sentiment.
- **Pollster Rating:** Polls from highly rated pollsters receive higher weights to reduce the impact of bias.

## Survey Implementation 
The survey will be created and administered through Google Forms for efficient data collection and secure storage. The main survey sections and example questions are outlined below. 

Access the survey: [Google Form](https://forms.gle/F1v3gswDjra8YLP79)

**Survey Overview** \
**Title:** 2024 US Presidential Election Poll \
**Purpose:** To collect public opinions and forecast election outcomes. \
**Estimated Time:** Less than 5 minutes \
**Confidentiality:** Responses are anonymous and used solely for research 

Consent to Participate
- I confirm that I understand the purpose of the survey, and I agree to participate.

1. What is your age group?
- Under 18
- 18-24
- 25-34
- 35-44
- 45-54
- 55-64
- 65-74
- 75 and older

2. What is your gender identity?
- Male
- Female
- Non-binary
- Prefer not to disclose

3. What is the highest level of education you have completed?
- Less than high school
- High school diploma or GED
- Some college, no degree
- Associate’s degree
- Bachelor’s degree
- Master’s degree
- Doctorate or professional degree (e.g., MD, JD, PhD)

4. Are you currently registered to vote in the United States?
- Yes
- No, but I am eligible and plan to register before the election
- No, and I am not planning to register
- No, I am not eligible to vote in the United States

5. What is your current political affiliation?
- Republican
- Democrat
- Independent 
- Libertarian
- Green Party
- Prefer not to disclose

6. How would you describe your political views?
- Very conservative
- Somewhat conservative
- Moderate
- Somewhat liberal
- Very liberal
- Prefer not to disclose

7. Who do you currently support in the 2024 U.S. presidential election?
- Donald Trump (Republican)
- Kamala Harris (Democrat)
- Undecided
- Other

8. How likely are you to vote in the 2024 presidential election?
- Extremely likely 
- Very likely 
- Moderately likely 
- Somewhat likely 
- Not at all likely 

## Budget Breakdown

-   **Online Recruitment** : \$25,000
-   **RDD Recruitment**: \$30,000
-   **Incentives for Participants**: \$20,000
-   **Data Processing & Validation**: \$15,000
-   **Miscellaneous Expenses**: \$10,000

**Total**: \$100,000

## Conclusion

This survey methodology uses stratified sampling, multi-channel recruitment, and data validation procedures to ensure accurate forecasting of the 2024 US Presidential Election. Through poll-of-polls aggregation, it offers a stable and reliable prediction by mitigating fluctuations in individual polls. The design aims to balance accuracy, inclusivity, and efficiency within a $100K budget, offering meaningful insights into voter behavior and sentiment. 

\newpage

# References
